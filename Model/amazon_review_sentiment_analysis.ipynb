{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10163274,
          "sourceType": "datasetVersion",
          "datasetId": 6276018
        },
        {
          "sourceId": 10165502,
          "sourceType": "datasetVersion",
          "datasetId": 6277506
        },
        {
          "sourceId": 10176632,
          "sourceType": "datasetVersion",
          "datasetId": 6285701
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Aim\n",
        "\n",
        "Use DistilBERT to capture the sentiment of users for a service/ product based on their review description.\n",
        "\n",
        "**Core Problem:** From this project we can see that understanding product sentiment based on the ratings alone ( out of 5 ) becomes somewhat of a challenge. This is because different users that have rated 3 stars to the product, may not share the same sentiment for the product.\n",
        "\n",
        "This difference in sentiment occurs because, when most of us take a look at 3 stars, we think of a 'Decent' product with a few minor complaints/ issues. But as it turns out, some portion of users rate 3 stars even though their description clearly shows a 'Bad' product that has a major complaints/ issues which should not be categorized as a 'Decent' product rated at 3 stars.\n",
        "\n",
        "**Solution:** Train and Fine-Tune our NLP model on the review descriptions left by users, and use the ratings ( out of 5 ) as 3 different target classes:\n",
        "- 1-2 star == Negative Class\n",
        "- 3 star == Neutral Class\n",
        "- 4-5 star == Positive Class\n",
        "\n",
        "Although the ground truth may be distorted because of the Neutral class not being consistent, we can still train our model to get a fair undestanding of the Neutral class which corresponds to a 'Decent' product with minor complaints/ issues. This is possible because most Neutral rated samples are of the 'Decent' type, so our model can still learn to make that distinction along with the help of learning what 'Negative' and 'Positive' classes imply."
      ],
      "metadata": {
        "id": "ULLjMLnSWntA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting Drive and Libraries\n"
      ],
      "metadata": {
        "id": "usxb4cuFr-mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "xr0C6Au5iyXL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "wRinqjC-lmB5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-10T22:25:06.951477Z",
          "iopub.execute_input": "2024-12-10T22:25:06.951952Z",
          "iopub.status.idle": "2024-12-10T22:25:18.904849Z",
          "shell.execute_reply.started": "2024-12-10T22:25:06.951906Z",
          "shell.execute_reply": "2024-12-10T22:25:18.903765Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "#from langdetect import detect\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.optim.lr_scheduler import StepLR\n"
      ],
      "metadata": {
        "id": "aaf4PXkqjy5q",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:09.411314Z",
          "iopub.execute_input": "2024-12-12T09:45:09.411569Z",
          "iopub.status.idle": "2024-12-12T09:45:25.595489Z",
          "shell.execute_reply.started": "2024-12-12T09:45:09.411542Z",
          "shell.execute_reply": "2024-12-12T09:45:25.594627Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "HMzycEPRupVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Datasets/amazon_reviews/amazon_reviews.tsv\""
      ],
      "metadata": {
        "id": "LU4OSgo2rsFw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting data into df\n",
        "\n",
        "df = pd.read_csv(data_path, sep='\\t', on_bad_lines='skip')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "E3rToLzavVQK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PRE PROCESSING\n"
      ],
      "metadata": {
        "id": "8wXsx6TmWntg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting relevant columns\n",
        "df_relevant = df[['star_rating', 'review_body']]\n",
        "\n",
        "# removing nan values\n",
        "df_noNAN = df_relevant.dropna(subset=['star_rating','review_body'])\n",
        "\n",
        "# removing duplicates\n",
        "df_no_duplicates = df_noNAN.drop_duplicates(subset=['review_body'], keep='first')\n",
        "\n",
        "# converting reviews rating column to integer\n",
        "df_no_duplicates['star_rating'] = df_no_duplicates['star_rating'].astype('int32')\n",
        "\n",
        "# limiting our size of reviews for computational efficieny and have similar data structure.\n",
        "df_no_duplicates['review_length'] = df_no_duplicates['review_body'].apply(len)\n",
        "\n",
        "threshold_length = 100\n",
        "\n",
        "df_filtered = df_no_duplicates[df_no_duplicates['review_length'] <= threshold_length]\n",
        "\n",
        "# Converting ratings into positive, neutral and negative...\n",
        "def categorize_rating(rating):\n",
        "    if rating in [4, 5]:\n",
        "        return 'positive'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "df_filtered['reviews.category'] = df_filtered['star_rating'].map(categorize_rating)\n",
        "\n",
        "# Using undersampling to deal with imbalanced data\n",
        "sample_size = 1000\n",
        "\n",
        "# Sampling from each rating category\n",
        "positive_samples = df_filtered[df_filtered['reviews.category'] == 'positive'].sample(n=sample_size, random_state=42)\n",
        "neutral_samples = df_filtered[df_filtered['reviews.category'] == 'neutral'].sample(n=sample_size, random_state=42)\n",
        "negative_samples = df_filtered[df_filtered['reviews.category'] == 'negative'].sample(n=sample_size, random_state=42)\n",
        "\n",
        "df_sampled = pd.concat([positive_samples, neutral_samples, negative_samples], ignore_index=True)"
      ],
      "metadata": {
        "id": "9E46bZjN1KMB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only english language reviews\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "df_sampled['language'] = df_sampled['review_body'].progress_apply(detect_language)\n",
        "\n",
        "\n",
        "df_english = df_sampled[df_sampled['language'] == 'en']"
      ],
      "metadata": {
        "id": "PBqYkIxOmseF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## saving/ retrieving the csv at our checkpoint\n",
        "\n",
        "# df_english.to_csv('df_english.csv')\n",
        "\n",
        "df_english = pd.read_csv('/kaggle/input/amazon-9k/df_english_9000.csv')\n",
        "df_english.drop('Unnamed: 0', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "7dMnzE_nzFLe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:25.600255Z",
          "iopub.execute_input": "2024-12-12T09:45:25.600591Z",
          "iopub.status.idle": "2024-12-12T09:45:25.649577Z",
          "shell.execute_reply.started": "2024-12-12T09:45:25.600552Z",
          "shell.execute_reply": "2024-12-12T09:45:25.648729Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our labels\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_english['label'] = label_encoder.fit_transform(df_english['reviews.category'])\n",
        "\n",
        "## Output: {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "\n",
        "labels = df_english['label'].tolist()\n"
      ],
      "metadata": {
        "id": "9NNu22gF4YrG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:25.651293Z",
          "iopub.execute_input": "2024-12-12T09:45:25.651566Z",
          "iopub.status.idle": "2024-12-12T09:45:25.657791Z",
          "shell.execute_reply.started": "2024-12-12T09:45:25.651540Z",
          "shell.execute_reply": "2024-12-12T09:45:25.656929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data into train and test\n",
        "\n",
        "X = df_english['review_body']\n",
        "y = df_english['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "x7Toy_K2cQDp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:25.658735Z",
          "iopub.execute_input": "2024-12-12T09:45:25.659582Z",
          "iopub.status.idle": "2024-12-12T09:45:25.668687Z",
          "shell.execute_reply.started": "2024-12-12T09:45:25.659544Z",
          "shell.execute_reply": "2024-12-12T09:45:25.667682Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Input Data\n"
      ],
      "metadata": {
        "id": "MA-4aon_2tGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing our inputs\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_data(texts):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        texts.tolist(),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(X_train)\n",
        "test_encodings = tokenize_data(X_test)"
      ],
      "metadata": {
        "id": "O6wPzy7h2na6",
        "outputId": "c334367b-e158-4959-d265-bae6a1d90c16",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:25.669713Z",
          "iopub.execute_input": "2024-12-12T09:45:25.670057Z",
          "iopub.status.idle": "2024-12-12T09:45:26.914423Z",
          "shell.execute_reply.started": "2024-12-12T09:45:25.670022Z",
          "shell.execute_reply": "2024-12-12T09:45:26.913489Z"
        },
        "colab": {
          "referenced_widgets": [
            "7c6e614deb004afb81c845a2d027d830",
            "714257e998a3451d98110089dd1f52ff",
            "4e48955aeb17444db2ba3c4faf28c64d",
            "a5dea1afdc344111b0249b9e7d020735"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c6e614deb004afb81c845a2d027d830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "714257e998a3451d98110089dd1f52ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e48955aeb17444db2ba3c4faf28c64d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5dea1afdc344111b0249b9e7d020735"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data into PyTorch Dataset"
      ],
      "metadata": {
        "id": "wFVVblj_Wntv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transforming our tokenized data and labels into dataset objects for PyTorch\n",
        "\n",
        "class SentimentData(Dataset):\n",
        "    def __init__(self, encodings, labels, sentences):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        item['sentence'] = self.sentences[idx]\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "2-UsrNEh5o2N",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:26.915734Z",
          "iopub.execute_input": "2024-12-12T09:45:26.916388Z",
          "iopub.status.idle": "2024-12-12T09:45:26.922492Z",
          "shell.execute_reply.started": "2024-12-12T09:45:26.916342Z",
          "shell.execute_reply": "2024-12-12T09:45:26.921592Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting labels to list\n",
        "y_traini = y_train.tolist()\n",
        "y_testi = y_test.tolist()\n",
        "\n",
        "# Coverting Actual sentences into list\n",
        "\n",
        "X_traini = X_train.tolist()\n",
        "X_testi = X_test.tolist()\n",
        "\n",
        "# Create the datasets for training and testing\n",
        "train_dataset = SentimentData(train_encodings, y_traini, X_traini)\n",
        "test_dataset = SentimentData(test_encodings, y_testi, X_testi)"
      ],
      "metadata": {
        "id": "DNXcBNtUZ5_o",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:26.923377Z",
          "iopub.execute_input": "2024-12-12T09:45:26.923613Z",
          "iopub.status.idle": "2024-12-12T09:45:26.941663Z",
          "shell.execute_reply.started": "2024-12-12T09:45:26.923588Z",
          "shell.execute_reply": "2024-12-12T09:45:26.940811Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataLoader for training and testing\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "yW4ngrMoa4ry",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:45:26.942580Z",
          "iopub.execute_input": "2024-12-12T09:45:26.942895Z",
          "iopub.status.idle": "2024-12-12T09:45:26.950223Z",
          "shell.execute_reply.started": "2024-12-12T09:45:26.942859Z",
          "shell.execute_reply": "2024-12-12T09:45:26.949545Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing our DistilBert Model ( base - uncased )"
      ],
      "metadata": {
        "id": "qWX4lfdcWntx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing our model\n",
        "\n",
        "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "MN2wZiFszssi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:14.589523Z",
          "iopub.execute_input": "2024-12-12T09:54:14.589787Z",
          "iopub.status.idle": "2024-12-12T09:54:14.702282Z",
          "shell.execute_reply.started": "2024-12-12T09:54:14.589762Z",
          "shell.execute_reply": "2024-12-12T09:54:14.701681Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our model...\n",
        "\n",
        "class Bert_Flow(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model):\n",
        "\n",
        "        super(Bert_Flow, self).__init__()\n",
        "\n",
        "        self.bert = bert_model\n",
        "\n",
        "        self.drop1 = nn.Dropout(0.1)\n",
        "        self.linear1 = nn.Linear(768,512)\n",
        "        self.relu1 =  nn.ReLU()\n",
        "        self.drop2 = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(512,3)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        cls_hs = last_hidden_state[:, 0, :]\n",
        "\n",
        "        x = self.drop1(cls_hs)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "MLaqI1sLf_7_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:15.037536Z",
          "iopub.execute_input": "2024-12-12T09:54:15.038321Z",
          "iopub.status.idle": "2024-12-12T09:54:15.043943Z",
          "shell.execute_reply.started": "2024-12-12T09:54:15.038290Z",
          "shell.execute_reply": "2024-12-12T09:54:15.043033Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# pass the pre-trained BERT to our custom architecture\n",
        "model = Bert_Flow(distilbert)\n",
        "\n",
        "# Pushing model into GPU if exists\n",
        "model = model.to(device)\n",
        "\n",
        "# Instantializing Adam optimizer\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Instantializing Crossetropy loss\n",
        "\n",
        "CEloss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "w7GtpAedV0-q",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:15.533840Z",
          "iopub.execute_input": "2024-12-12T09:54:15.534176Z",
          "iopub.status.idle": "2024-12-12T09:54:15.633459Z",
          "shell.execute_reply.started": "2024-12-12T09:54:15.534146Z",
          "shell.execute_reply": "2024-12-12T09:54:15.632831Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Train, Helper, and Test Functions"
      ],
      "metadata": {
        "id": "aMhYKiR4Wntz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our Training function\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print(f'Batch {step} of {len(train_loader)}.')\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = CEloss(preds, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "\n",
        "        total_preds.append(preds)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "\n",
        "    predicted_labels = np.argmax(total_preds, axis=1)\n",
        "    all_labels = np.array(all_labels)\n",
        "    accuracy = np.mean(predicted_labels == all_labels)\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "vNuyMBL_mnMM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:16.512332Z",
          "iopub.execute_input": "2024-12-12T09:54:16.512670Z",
          "iopub.status.idle": "2024-12-12T09:54:16.519931Z",
          "shell.execute_reply.started": "2024-12-12T09:54:16.512639Z",
          "shell.execute_reply": "2024-12-12T09:54:16.518970Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our helper function that helps create our final results DataFrame\n",
        "\n",
        "def analyze_neutral_predictions():\n",
        "\n",
        "    # Initializing an empty list to store the data for the DataFrame\n",
        "    data = []\n",
        "\n",
        "    # Iterating through the data\n",
        "    for i in range(len(final_preds)):\n",
        "        sentence = sentences_list[i]\n",
        "        predicted_class = final_preds[i]\n",
        "        true_class = final_labels[i]\n",
        "\n",
        "        # Maping true class label to its textual representation\n",
        "        true_class_label = ['negative', 'neutral', 'positive'][true_class]\n",
        "\n",
        "\n",
        "        # Determining the class label for the DataFrame\n",
        "        if predicted_class == 1:\n",
        "            class_label = 'neutral'\n",
        "        elif predicted_class == 0:\n",
        "            class_label = 'negative'\n",
        "        elif predicted_class == 2:\n",
        "            class_label = 'positive'\n",
        "\n",
        "        # Appending the sentence and its class label to the data\n",
        "        data.append((sentence, class_label, true_class_label))\n",
        "\n",
        "    # Creating a pandas DataFrame with the collected data\n",
        "    df = pd.DataFrame(data, columns=['Sentences', 'Predicted Class', 'True Class'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:17.005939Z",
          "iopub.execute_input": "2024-12-12T09:54:17.006227Z",
          "iopub.status.idle": "2024-12-12T09:54:17.012177Z",
          "shell.execute_reply.started": "2024-12-12T09:54:17.006197Z",
          "shell.execute_reply": "2024-12-12T09:54:17.011291Z"
        },
        "id": "TBzKzl_0Wnt1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our Evaluation function\n",
        "\n",
        "def evaluate():\n",
        "    print(\"\\nEvaluating...\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_preds = []\n",
        "    all_labels = []\n",
        "    sentences_list = []\n",
        "\n",
        "    for step, batch in enumerate(test_loader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "\n",
        "            print(f'  Batch {step} of {len(test_loader)}')\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        sentences = batch['sentence']\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        sentences_list.extend(sentences)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            preds = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = CEloss(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "\n",
        "    all_labels = np.array(all_labels)                                    # These are the Actual Labels corresponding to each prediction\n",
        "    total_preds = np.concatenate(total_preds, axis=0)                    # These are the Logits predicted\n",
        "    predicted_labels = np.argmax(total_preds, axis=1)                    # Converting the logits into labels\n",
        "    probabilities = torch.softmax(torch.tensor(total_preds), dim = 1)    # These are predicted probabilities\n",
        "    accuracy = np.mean(predicted_labels == all_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return avg_loss, accuracy, predicted_labels, total_preds ,all_labels, sentences_list, probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "y2mtPIRSxgHN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:17.472675Z",
          "iopub.execute_input": "2024-12-12T09:54:17.473097Z",
          "iopub.status.idle": "2024-12-12T09:54:17.481187Z",
          "shell.execute_reply.started": "2024-12-12T09:54:17.473057Z",
          "shell.execute_reply": "2024-12-12T09:54:17.480345Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running our model..."
      ],
      "metadata": {
        "id": "tFYef2ftWnt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# running the model...\n",
        "\n",
        "# Setting initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "final_preds = 0\n",
        "final_labels = 0\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    print(f'\\nEpoch {epoch + 1} / {epochs}')\n",
        "\n",
        "    train_loss, train_accuracy = train()\n",
        "\n",
        "    valid_loss, valid_accuracy, pred_labels, total_preds ,all_labels, sentences_list, probabilities = evaluate()\n",
        "\n",
        "    # Saving the best model based on validation loss\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    if epoch == epochs - 1:\n",
        "\n",
        "        # final_preds are the predicted LABELS NmPy array\n",
        "        # final_labels are the Actual LABLES NmPy array\n",
        "        # final_logits are the predicted logits\n",
        "        final_preds, final_labels, final_logits = pred_labels, all_labels, total_preds\n",
        "        data_results = analyze_neutral_predictions()\n",
        "\n",
        "\n",
        "    # Printing losses and accuracies\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}, Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}, Validation Accuracy: {valid_accuracy * 100:.2f}%')\n",
        ""
      ],
      "metadata": {
        "id": "IUnVu2kozRmS",
        "outputId": "7f0824a7-c05e-495f-aeaa-9dd5457fe3ff",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:54:18.600350Z",
          "iopub.execute_input": "2024-12-12T09:54:18.600650Z",
          "iopub.status.idle": "2024-12-12T10:00:47.375841Z",
          "shell.execute_reply.started": "2024-12-12T09:54:18.600624Z",
          "shell.execute_reply": "2024-12-12T10:00:47.374829Z"
        },
        "colab": {
          "referenced_widgets": [
            "a22e4233136746bc9fc43587b6d79849"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a22e4233136746bc9fc43587b6d79849"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nEpoch 1 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 1.054, Training Accuracy: 48.75%\nValidation Loss: 0.941, Validation Accuracy: 67.04%\n\nEpoch 2 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.845, Training Accuracy: 66.64%\nValidation Loss: 0.752, Validation Accuracy: 69.98%\n\nEpoch 3 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.733, Training Accuracy: 70.43%\nValidation Loss: 0.691, Validation Accuracy: 70.86%\n\nEpoch 4 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.684, Training Accuracy: 72.24%\nValidation Loss: 0.664, Validation Accuracy: 71.97%\n\nEpoch 5 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.651, Training Accuracy: 73.56%\nValidation Loss: 0.652, Validation Accuracy: 72.44%\n\nEpoch 6 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.628, Training Accuracy: 73.94%\nValidation Loss: 0.645, Validation Accuracy: 72.27%\n\nEpoch 7 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.610, Training Accuracy: 74.96%\nValidation Loss: 0.639, Validation Accuracy: 72.39%\n\nEpoch 8 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.596, Training Accuracy: 75.84%\nValidation Loss: 0.640, Validation Accuracy: 72.44%\n\nEpoch 9 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.581, Training Accuracy: 76.57%\nValidation Loss: 0.640, Validation Accuracy: 71.86%\n\nEpoch 10 / 10\nBatch 50 of 426.\nBatch 100 of 426.\nBatch 150 of 426.\nBatch 200 of 426.\nBatch 250 of 426.\nBatch 300 of 426.\nBatch 350 of 426.\nBatch 400 of 426.\n\nEvaluating...\n  Batch 50 of 107\n  Batch 100 of 107\n\nTraining Loss: 0.564, Training Accuracy: 76.76%\nValidation Loss: 0.640, Validation Accuracy: 72.56%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving out dataframe/ Retreiving our dataframe\n",
        "\n",
        "#data_results.to_csv('neutral.csv')\n",
        "\n",
        "#data_results = pd.read_csv('/kaggle/input/data-results/data_results.csv')\n",
        "#data_results.drop('Unnamed: 0', axis = 1, inplace = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T05:24:59.910192Z",
          "iopub.execute_input": "2024-12-12T05:24:59.910644Z",
          "iopub.status.idle": "2024-12-12T05:24:59.945362Z",
          "shell.execute_reply.started": "2024-12-12T05:24:59.910607Z",
          "shell.execute_reply": "2024-12-12T05:24:59.944162Z"
        },
        "id": "iNZUpHoIWnt5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating our data ( 9000 samples )"
      ],
      "metadata": {
        "id": "5EEq0rF4Wnt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic evaluation metrics..."
      ],
      "metadata": {
        "id": "VJx6q6ePWnt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision, Recall, and F1 Scores for each class.\n",
        "\n",
        "print(classification_report(final_labels, final_preds, target_names=['Negative', 'Neutral', 'Positive']))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T10:07:21.205314Z",
          "iopub.execute_input": "2024-12-12T10:07:21.205946Z",
          "iopub.status.idle": "2024-12-12T10:07:21.221553Z",
          "shell.execute_reply.started": "2024-12-12T10:07:21.205894Z",
          "shell.execute_reply": "2024-12-12T10:07:21.220735Z"
        },
        "id": "w2NiEYaSWnuN",
        "outputId": "67cc33dc-95b0-48dc-b0b1-3968ff7a55bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n    Negative       0.73      0.77      0.75       541\n     Neutral       0.65      0.57      0.61       571\n    Positive       0.78      0.84      0.80       590\n\n    accuracy                           0.73      1702\n   macro avg       0.72      0.73      0.72      1702\nweighted avg       0.72      0.73      0.72      1702\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Critical Observation..."
      ],
      "metadata": {
        "id": "qr2AGzqPWnuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we observe that our model is doing well for both Positive and Negative samples, although the performance is not very good for Neutral samples\n",
        "\n",
        "This may be because for 'Positive' (rated 4/5 out of 5), and 'Negative' (rated 1/2 out of 5) samples, the sentiment of the person is very clear, but the 'Neutral' samples ( rated 3 out of 5 ) are not all rated with the same sentiment in mind.\n",
        "\n",
        "This means that Neutral samples may not capture the true sentiment of the people in that category. Let us see some examples of 'Neutral' samples that were rated by people on Amazon...\n",
        "\n",
        "Set1 (Neutral rated samples)-\n",
        "- 'Story concluded to quickly and was anti climatic, but a good read.'                                             # MODEL DECIDED ITS NEUTRAL  ( minor complaint )\n",
        "\n",
        "- 'Not bad, good solid tour of Italy. Ending could have been more exciting....'                                    # MODEL DECIDED ITS NEUTRAL  ( minor complaint )\n",
        "\n",
        "- 'I liked the book but I believe it could use more action and details! BOOM SAUCE!  ;) ;) ;) :) :) :)'            # MODEL DECIDED ITS NEUTRAL  ( minor complaint )\n",
        "\n",
        "- 'Good book, loved it, just dragged on a tiny bit'                                                                # MODEL DECIDED ITS POSITIVE  \n",
        "\n",
        "- 'As good as her other book, The Husband's Secret. A good read!'                                                  # MODEL DECIDED ITS POSITIVE\n",
        "  \n",
        "\n",
        "Set2 (Neutral rated sample )-\n",
        "- 'Keeps shutting down. Not please with this app.                                                                 # MODEL DECIDED ITS NEGATIVE  ( major complaint )'\n",
        "\n",
        "- 'Takes too long for energy to recharge'                                                                          # MODEL DECIDED ITS NEGATIVE  ( major complaint )\n",
        "\n",
        "- 'Not what I really wanted didn't fit my ear! But cost time and Money 2 send back!'                               # MODEL DECIDED ITS NEGATIVE  ( major complaint )\n",
        "\n",
        "- 'I can not really rate the movie.  The very poor streaming of it made it unbearable to watch.'                   # MODEL DECIDED ITS NEGATIVE  ( major complaint )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It looks as if Set1 is more like 'Decent', where it has a small hiccup that the reviewer does not mind. Set2 is more like 'Bad', where there was a greater level of critique.\n",
        "\n",
        "But both users have rated it neutral, which goes against the inuitive concept of 'Neutral Rating' which we think of as 'Decent' with few minor complains/ issues.\n"
      ],
      "metadata": {
        "id": "n2Wt6YpbWnuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "7buVTIxuzMIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that our model does infact perform quite well to distinguish Set1 as Positive mostly ( and Neutral sometimes ) and Set 2 as Negative mostly ( and Neutral sometimes ).\n",
        "\n",
        "So we can conclude that if our definition of Neutral is 'Decent' with a few minor complaints/ issues, and Negative relates to a more major complaint/ issue, then we can see that our model does quite well.\n",
        "\n",
        "Since the ground truth is quite distorted with the Neutral class, it seems as though the evaluation scores are not the best, when in fact, our model has much higher accuracy for each class, when the review description is considered. ."
      ],
      "metadata": {
        "id": "-q9mGD_6zPYB"
      }
    }
  ]
}